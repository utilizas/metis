---
<<<<<<< HEAD
title: "Sobre el potencial de los agentes IA especializados en textos filosóficos"
=======
title: "Sobre el potencial de los agentes IA especializados [ejemplo/prueba]"
>>>>>>> 2283730ce0201792581eba93299bc4b5e1668029
author: "M. Luzón "
date: "2025-05-31"
tags: IA agencial, interpretación, analítica textual,
affiliation: "ULPGC"
---

# Introducción

## Abstract

Progress in the development of autonomous artificial intelligence systems (AAIS) has been exponential, expanding the range of complex tasks they can automate. However, the growing diversity of contexts in which these systems are able to make decisions and perform actions without human intervention presents new challenges in evaluating their true level of autonomy (Festor et al. 2021; Burnell et al. 2023). 

Traditional benchmarks focus on specific tasks and do not capture the complexity of the autonomy requirement in real-world scenarios, with the risk that their results may bias the performance assessment of these systems and incentivise demand that overestimates their potential (Shen et al. 2023). The problem is compounded by the reluctance of many companies to make public the benchmarks against which they train their state-of-the-art models (Burnell et al. 2023).

My proposal includes a set of criteria and benchmarks to assess high leveles of autonomy in AI systems tailored for various high-skill professional tasks. Autonomy enables AI systems to adapt, decide, and learn from experiences in changing conditions. This approach includes access to information and essential context for skilled operators. Typical benchmarks are crafted using a mix of techniques, including simulation, reinforcement learning, and data analysis. The goal is to ensure a robust and adaptable evaluation framework, applicable to diverse AI implementations ranging from autonomous vehicles and medical support to educational educational reinforcement for struggling students (Cowls et al. 2021).

Real-world evaluation presents a complex challenge due to its dynamic and multifaceted nature.  Obtaining relevant benchmarks applicable to practical situations becomes difficult. For instance, autonomous driving systems are evaluated on their ability to navigate complex traffic, respond to unexpected events, and learn from past interactions to improve decision-making (Neelofar & Aleti 2023). In contrast, an AAIS designed for monitoring chronic disease patients faces a broader set of demanding requirements. These include accuracy in detecting changes, reliable alerts, robust security and patient privacy protection, seamless integration with existing healthcare systems, the ability to learn and adapt, explainability of its decisions, and adherence to ethical and legal considerations. Additionally, the system must demonstrate a positive impact on quality of care, reduced workload for healthcare professionals, and patient acceptance (Festor et al. 2021). While the first set of requirements can be assessed through relatively simple metrics, the second necessitates contextual elements, which are inherently more complex. These elements include working conditions, legal frameworks, and the culture surrounding healthcare system usage. 

The proposed benchmarks offer a multi-level assessment of AI system autonomy, from simple task execution to complex, adaptive learning in dynamic environments. They provide a metric for gauging AI development and pinpointing areas for enhancement (Xu et al. 2022). By establishing clear and quantifiable standards, robust benchmarks can guide the development of safer, more reliable, and truly autonomous AI systems. This, in turn, fosters better understanding among users and stakeholders regarding the capabilities, limitations, and trustworthiness of these systems (Lehman et al. 2020).

*Keywords*: Autonomy, Artificial Intelligence, Benchmarks, Reinforcement Learning, Simulation.  

## References

1. Festor, P. et al. (2021). Levels of Autonomy and Safety Assurance for AI-Based Clinical Decision Systems (pp. 291–296). https://doi.org/10.1007/978-3-030-83906-2_24.  
2. Burnell, R. et al. (2023). Rethink reporting of evaluation results in AI. Science, 380(6641), 136–138. https://doi.org/10.1126/science.adf6369.  
3. Shen, Y. et al. (2023). Taskbench: Benchmarking Large Language Models for Task Automation. Computation and Language. https://doi.org/10.48550/arXiv.2311.18760.  
4. Xu, Chejian et al. (2022) SafeBench: A Benchmarking Platform for Safety Evaluation of Autonomous Vehicles. 36th Conference on Neural Information Processing Systems (NeurIPS 2022) Track on Datasets and Benchmarks. 1-16. https://papers.nips.cc/paper_files/paper/2022/file/a48ad12d588c597f4725a8b84af647b5-Paper-Datasets_and_Benchmarks.pdf.  
5. Neelofar, N., & Aleti, A. (2023, 14 noviembre). Towards Reliable AI: Adequacy Metrics for Ensuring the Quality of System-level Testing of Autonomous Vehicles. arXiv.org. https://arxiv.org/abs/2311.08049.  
6. Cowls, J., Tsamados, A., Taddeo, M., & Floridi, L. (2021). A definition, benchmark and database of AI for social good initiatives. Nature Machine Intelligence, 3(2), 111–115. https://doi.org/10.1038/s42256-021-00296-0.  
7. Lehman, J. et al. (2020). The Surprising Creativity of Digital Evolution. Artificial Life, 26(2), 274-306. https://doi.org/10.1162/artl_a_00319.  
